import os
import re
import json
import time
from typing import List, Dict
from pathlib import Path
from datetime import datetime

import requests
import numpy as np
import torch
from bs4 import BeautifulSoup
from pymilvus import connections, Collection, utility, FieldSchema, CollectionSchema, DataType
from sentence_transformers import CrossEncoder
from transformers import AutoTokenizer, AutoModel
import streamlit as st
from rank_bm25 import BM25Okapi
import jieba
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

#ä¸ªäººé…ç½®
your_uri="uri_path"
your_token="token_name"
your_api="secret_api"
file_path=r"your_filepath"

# äº‘ç«¯Milvusè¿æ¥ï¼ˆå»ºè®®å°†æ•æ„Ÿä¿¡æ¯ç§»åˆ°ç¯å¢ƒå˜é‡ï¼‰
connections.connect(
    uri=your_uri,
    token=your_token
)

# ======================
# ç³»ç»Ÿé…ç½®
# ======================
class SystemConfig:
    EMBEDDING_MODEL = "BAAI/bge-small-zh-v1.5"
    EMBEDDING_DIM = 512
    EMBEDDING_DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
    MAX_TEXT_LENGTH = 10000
    CHUNK_SIZE = 512
    CHUNK_OVERLAP = 50
    MAX_DOC_ID_LENGTH = 256
    COLLECTION_NAME = "medical_rag"
    MEDICAL_TERMS_PATH = "medical_terms.json"

    # æ–°å¢reranké…ç½®
    RERANK_CONFIG = {
        "enable_rerank": True,
        "cross_encoder_model": "cross-encoder/ms-marco-MiniLM-L-6-v2",
        "initial_recall_num": 20,
        "final_top_k": 5,
        "score_weights": {
            "vector": 0.5,
            "keyword": 0.3,
            "rerank": 0.2,
            "diversity": -0.1
        },
        "diversity_threshold": 0.7,
        "min_rerank_score": 0.2
    }


# ======================
# ç¡…åŸºæµåŠ¨APIå®¢æˆ·ç«¯
# ======================
class SiliconFlowClient:
    def __init__(self):
        self.base_url = "https://api.siliconflow.cn/v1/chat/completions"
        self.api_key = your_api
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
        self.session = requests.Session()
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
            allowed_methods=["POST"]
        )
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("https://", adapter)

    def generate_answer(self, question: str, context: List[str]) -> dict:
        system_prompt = """ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç–—åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹åŒ»å­¦èµ„æ–™å›ç­”é—®é¢˜ï¼š
{}

å›ç­”è¦æ±‚ï¼š
1. æ ¸å¿ƒå®šä¹‰ç®€æ˜æ‰¼è¦
2. å…³é”®ç‰¹å¾åˆ†ç‚¹åˆ—å‡ºï¼ˆæœ€å¤š3ç‚¹ï¼‰
3. å¿…é¡»æ ‡æ³¨æ•°æ®æ¥æº
4. ä½¿ç”¨ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾""".format('\n'.join([f"[æ¥æº:{i + 1}] {text}" for i, text in enumerate(context)]))

        payload = {
            "model": "Qwen/Qwen3-30B-A3B",
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": question}
            ],
            "temperature": 0.3,
            "max_tokens": 1000
        }

        try:
            response = self.session.post(
                self.base_url,
                headers=self.headers,
                json=payload,
                timeout=(10, 30)
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.Timeout as e:
            st.error("è¯·æ±‚è¶…æ—¶ï¼Œå»ºè®®ï¼š1. æ£€æŸ¥ç½‘ç»œè¿æ¥ 2. ç®€åŒ–é—®é¢˜ 3. ç¨åé‡è¯•")
            return {"error": "APIè¯·æ±‚è¶…æ—¶"}
        except requests.exceptions.RequestException as e:
            st.error(f"APIè¯·æ±‚å¤±è´¥: {str(e)}")
            return {"error": str(e)}


# ======================
# ç–¾ç—…å…³é”®è¯æå–å™¨
# ======================
class DiseaseExtractor:
    def __init__(self):
        self.medical_terms = self._load_medical_terms()
        self._initialize_jieba()

    def _load_medical_terms(self) -> dict:
        default_terms = {
            "diseases": [
                "é«˜è¡€å‹", "ç³–å°¿ç—…", "å† å¿ƒç—…", "è‰¾æ»‹ç—…", "æŠ‘éƒç—‡", "å¸•é‡‘æ£®",
                "ç™½è¡€ç—…", "è‚ºç‚", "è‚ç‚", "è‚¾ç‚", "èƒƒç‚", "è‚ ç‚", "çš®ç‚",
                "è‚ºç™Œ", "èƒƒç™Œ", "è‚ç™Œ", "ä¹³è…ºç™Œ", "å‰åˆ—è…ºç™Œ"
            ],
            "symptoms": [
                "å¤´ç—›", "å‘çƒ­", "å’³å—½", "å‘•å", "è…¹æ³»", "çš®ç–¹", "ç˜™ç—’",
                "ç–¼ç—›", "è‚¿èƒ€", "ä¹åŠ›", "çœ©æ™•", "å¿ƒæ‚¸", "æ°”çŸ­"
            ],
            "synonyms": {
                "ç³–å°¿ç—…": ["é«˜è¡€ç³–", "æ¶ˆæ¸´ç—‡"],
                "å† å¿ƒç—…": ["å¿ƒè‚Œç¼ºè¡€", "å¿ƒç»ç—›"],
                "é«˜è¡€å‹": ["è¡€å‹é«˜"]
            }
        }

        try:
            if Path(SystemConfig.MEDICAL_TERMS_PATH).exists():
                with open(SystemConfig.MEDICAL_TERMS_PATH, "r", encoding="utf-8") as f:
                    return json.load(f)
            return default_terms
        except:
            return default_terms

    def _initialize_jieba(self):
        for disease in self.medical_terms["diseases"]:
            jieba.add_word(disease)
        for symptom in self.medical_terms["symptoms"]:
            jieba.add_word(symptom)

    def extract(self, text: str) -> List[str]:
        words = jieba.lcut(text)
        diseases = [w for w in words if w in self.medical_terms["diseases"]]
        symptoms = [w for w in words if w in self.medical_terms["symptoms"]]

        compound_pattern = re.compile(
            r"([\u4e00-\u9fa5]{1,5}[\u4e00-\u9fa5]+(?:ç—…|ç—‡|ç‚|ç™Œ|ç˜¤|ç»“çŸ³|ç»¼åˆå¾|ç»¼åˆç—‡|ç´Šä¹±|è¡°ç«­|ä¸­æ¯’|æŸä¼¤))"
        )
        compounds = compound_pattern.findall(text)

        return list(set(diseases + symptoms + compounds))


# ======================
# æ–‡æ¡£é¢„å¤„ç†
# ======================
class MedicalDataPreprocessor:
    def __init__(self):
        self.disease_extractor = DiseaseExtractor()

    def clean_text(self, text: str) -> str:
        if not isinstance(text, str):
            return ""

        text = BeautifulSoup(text, 'html.parser').get_text(separator='\n')
        paragraphs = [p for p in text.split('\n') if len(p) > 15]
        return '\n'.join(paragraphs)[:SystemConfig.MAX_TEXT_LENGTH]

    def get_doc_id(self, file_path: str) -> str:
        fname = Path(file_path).name
        if len(fname) > SystemConfig.MAX_DOC_ID_LENGTH:
            return f"{fname[:8]}_{hash(file_path)[:8]}"
        return fname


# ======================
# æ–‡æ¡£åˆ†å—
# ======================
class MedicalDocumentChunker:
    def __init__(self):
        self.splitter = re.compile(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\n])')
        self.disease_extractor = DiseaseExtractor()

    def chunk_document(self, document: dict) -> List[dict]:
        content = document.get("content", "")
        sentences = [s.strip() for s in self.splitter.split(content) if s.strip()]

        chunks = []
        current_chunk = []
        current_len = 0

        for sent in sentences:
            sent_len = len(sent)
            if current_len + sent_len > SystemConfig.CHUNK_SIZE:
                if self._is_valid_chunk(current_chunk):
                    chunks.append(self._create_chunk(current_chunk, document["doc_id"], len(chunks)))
                current_chunk = current_chunk[-SystemConfig.CHUNK_OVERLAP:]
                current_len = sum(len(s) for s in current_chunk)

            current_chunk.append(sent)
            current_len += sent_len

        if self._is_valid_chunk(current_chunk):
            chunks.append(self._create_chunk(current_chunk, document["doc_id"], len(chunks)))

        return chunks

    def _create_chunk(self, sentences: List[str], doc_id: str, chunk_id: int) -> dict:
        text = ''.join(sentences)
        return {
            "text": text,
            "doc_id": doc_id,
            "chunk_id": chunk_id,
            "entities": self.disease_extractor.extract(text)
        }

    def _is_valid_chunk(self, sentences: List[str]) -> bool:
        return len(''.join(sentences)) >= 50


# ======================
# æ£€ç´¢æ¨¡å—(å®Œæ•´ä¿®æ”¹ç‰ˆ)
# ======================
class MedicalRetriever:
    def __init__(self, preprocessor: MedicalDataPreprocessor):
        self.preprocessor = preprocessor
        self.tokenizer, self.model, self.cross_encoder = load_models()
        self.collection = self._initialize_milvus()
        self.disease_extractor = DiseaseExtractor()
        self._initialize_reranker()
        self.similarity_model = None

    def _initialize_milvus(self):
        try:
            if not utility.has_collection(SystemConfig.COLLECTION_NAME):
                self._create_collection_with_index()
            else:
                self.collection = Collection(SystemConfig.COLLECTION_NAME)
                self._validate_collection_schema()

            self.collection.load()
            st.success("æˆåŠŸè¿æ¥äº‘ç«¯MilvusæœåŠ¡")
            return self.collection
        except Exception as e:
            st.error(f"Milvusè¿æ¥å¤±è´¥: {str(e)}")
            raise RuntimeError("æ— æ³•è¿æ¥MilvusæœåŠ¡") from e

    def _create_collection_with_index(self):
        fields = [
            FieldSchema(name="id", dtype=DataType.INT64, is_primary=True, auto_id=True),
            FieldSchema(name="doc_id", dtype=DataType.VARCHAR, max_length=SystemConfig.MAX_DOC_ID_LENGTH),
            FieldSchema(name="chunk_id", dtype=DataType.INT64),
            FieldSchema(name="text", dtype=DataType.VARCHAR, max_length=65535),
            FieldSchema(name="embedding", dtype=DataType.FLOAT_VECTOR, dim=SystemConfig.EMBEDDING_DIM)
        ]

        schema = CollectionSchema(fields, description="åŒ»ç–—æ–‡æ¡£å‘é‡åº“")
        self.collection = Collection(SystemConfig.COLLECTION_NAME, schema)

        index_params = {
            "index_type": "HNSW",
            "metric_type": "IP",
            "params": {"M": 16, "efConstruction": 200}
        }
        self.collection.create_index("embedding", index_params)

    def _validate_collection_schema(self):
        current_schema = self.collection.schema
        required_fields = {"doc_id", "text", "embedding"}
        if not required_fields.issubset({f.name for f in current_schema.fields}):
            raise ValueError("é›†åˆschemaä¸å…¼å®¹ï¼Œå»ºè®®åˆ é™¤é‡å»º")

    def _initialize_reranker(self):
        if SystemConfig.RERANK_CONFIG["enable_rerank"]:
            try:
                if self.cross_encoder is None:
                    self.cross_encoder = CrossEncoder(
                        SystemConfig.RERANK_CONFIG["cross_encoder_model"]
                    )
                self.similarity_model = AutoModel.from_pretrained(
                    "BAAI/bge-small-zh-v1.5",
                    device_map=SystemConfig.EMBEDDING_DEVICE
                )
                st.success("é‡æ’åºæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as e:
                st.warning(f"é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
                SystemConfig.RERANK_CONFIG["enable_rerank"] = False

    def _calculate_similarity(self, text1: str, text2: str) -> float:
        if self.similarity_model is None:
            return 0

        inputs = self.tokenizer(
            [text1, text2],
            padding=True,
            truncation=True,
            return_tensors="pt"
        ).to(SystemConfig.EMBEDDING_DEVICE)

        with torch.no_grad():
            outputs = self.similarity_model(**inputs)
            embeddings = outputs.last_hidden_state.mean(dim=1)
            sim = torch.cosine_similarity(embeddings[0], embeddings[1], dim=0)
            return float(sim)

    def _apply_diversity_penalty(self, results: List[dict]) -> List[dict]:
        if len(results) < 2:
            return results

        sim_matrix = np.zeros((len(results), len(results)))
        for i in range(len(results)):
            for j in range(i + 1, len(results)):
                sim = self._calculate_similarity(
                    results[i]["text"],
                    results[j]["text"]
                )
                sim_matrix[i][j] = sim
                sim_matrix[j][i] = sim

        for i in range(len(results)):
            similar_docs = np.where(
                sim_matrix[i] > SystemConfig.RERANK_CONFIG["diversity_threshold"]
            )[0]
            if len(similar_docs) > 0:
                max_score_idx = max(similar_docs, key=lambda x: results[x]["score"])
                if i != max_score_idx:
                    results[i]["diversity_penalty"] = (
                            SystemConfig.RERANK_CONFIG["score_weights"]["diversity"] *
                            max(sim_matrix[i][similar_docs])
                    )

        return results

    def _rerank_results(self, query: str, results: List[dict]) -> List[dict]:
        if not SystemConfig.RERANK_CONFIG["enable_rerank"] or len(results) < 2:
            return results[:SystemConfig.RERANK_CONFIG["final_top_k"]]

        # å…³é”®è¯å¢å¼º
        keywords = self.disease_extractor.extract(query)
        for res in results:
            res["keyword_score"] = sum(
                1 for kw in keywords
                if kw in res["text"] or any(kw in syn for syn in res.get("keywords", []))
            )

        # CrossEncoderé‡æ’åº
        pairs = [(query, res["text"]) for res in results]
        rerank_scores = self.cross_encoder.predict(pairs)

        max_rerank = max(rerank_scores) if max(rerank_scores) > 0 else 1
        for i, res in enumerate(results):
            res["rerank_score"] = float(rerank_scores[i] / max_rerank)
            if res["rerank_score"] < SystemConfig.RERANK_CONFIG["min_rerank_score"]:
                res["rerank_score"] = 0

        # å¤šæ ·æ€§æƒ©ç½š
        results = self._apply_diversity_penalty(results)

        # ç»¼åˆè¯„åˆ†
        max_vec = max(r["score"] for r in results) or 1
        max_kw = max(r["keyword_score"] for r in results) or 1

        for res in results:
            res["final_score"] = (
                    SystemConfig.RERANK_CONFIG["score_weights"]["vector"] * (res["score"] / max_vec) +
                    SystemConfig.RERANK_CONFIG["score_weights"]["keyword"] * (res["keyword_score"] / max_kw) +
                    SystemConfig.RERANK_CONFIG["score_weights"]["rerank"] * res["rerank_score"] +
                    res.get("diversity_penalty", 0)
            )

        return sorted(
            [r for r in results if r["final_score"] > 0],
            key=lambda x: x["final_score"],
            reverse=True
        )[:SystemConfig.RERANK_CONFIG["final_top_k"]]

    def retrieve(self, query: str, top_k: int = None) -> List[dict]:
        if top_k is None:
            top_k = SystemConfig.RERANK_CONFIG["final_top_k"]

        initial_recall = SystemConfig.RERANK_CONFIG["initial_recall_num"]

        query_embedding = self._embed_texts([query])[0]
        search_params = {"metric_type": "IP", "params": {"nprobe": 16}}

        results = self.collection.search(
            data=[query_embedding],
            anns_field="embedding",
            param=search_params,
            limit=initial_recall,
            output_fields=["text", "doc_id", "chunk_id"]
        )

        results = [{
            "text": hit.entity.get("text"),
            "doc_id": hit.entity.get("doc_id"),
            "chunk_id": hit.entity.get("chunk_id"),
            "score": hit.score,
            "keywords": self.disease_extractor.extract(hit.entity.get("text"))
        } for hit in results[0]]

        return self._rerank_results(query, results)

    def _embed_texts(self, texts: List[str]) -> List[List[float]]:
        inputs = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=512,
            return_tensors="pt"
        ).to(SystemConfig.EMBEDDING_DEVICE)

        with torch.no_grad():
            outputs = self.model(**inputs)
            embeddings = torch.mean(outputs.last_hidden_state, dim=1).cpu().numpy()

        return (embeddings / np.linalg.norm(embeddings, axis=1, keepdims=True)).tolist()

    def ingest_documents(self, file_paths: List[str]):
        chunker = MedicalDocumentChunker()

        for path in file_paths:
            try:
                with open(path, 'r', encoding='utf-8') as f:
                    content = f.read()

                doc_id = self.preprocessor.get_doc_id(path)
                clean_content = self.preprocessor.clean_text(content)

                document = {"doc_id": doc_id, "content": clean_content}
                chunks = chunker.chunk_document(document)

                texts = [c["text"] for c in chunks]
                embeddings = self._embed_texts(texts)

                entities = [
                    {"doc_id": c["doc_id"],
                     "chunk_id": c["chunk_id"],
                     "text": c["text"],
                     "embedding": emb}
                    for c, emb in zip(chunks, embeddings)
                ]

                batch_size = 100
                for i in range(0, len(entities), batch_size):
                    batch = entities[i:i + batch_size]
                    self.collection.insert([
                        [e["doc_id"] for e in batch],
                        [e["chunk_id"] for e in batch],
                        [e["text"] for e in batch],
                        [e["embedding"] for e in batch]
                    ])

            except Exception as e:
                st.error(f"æ–‡ä»¶å¤„ç†å¤±è´¥: {Path(path).name} - {str(e)}")

        self.collection.flush()
        st.success(f"æˆåŠŸåŠ è½½ {len(file_paths)} ä¸ªæ–‡æ¡£")


# ======================
# ä¸»ç³»ç»Ÿ (å®Œæ•´ä¿®æ”¹ç‰ˆ)
# ======================
class MedicalRAGSystem:
    def __init__(self):
        self.preprocessor = MedicalDataPreprocessor()
        self.silicon_client = SiliconFlowClient()
        try:
            self.retriever = MedicalRetriever(self.preprocessor)
            self.disease_extractor = DiseaseExtractor()
            self.bm25 = None
        except Exception as e:
            st.error(f"æ£€ç´¢ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise
        self.dialog_history = []
        self.max_history = 5

    def _build_context_prompt(self, question: str, context: List[str]) -> str:
        system_prompt = """ä½ æ˜¯ä¸€åä¸“ä¸šåŒ»ç–—åŠ©æ‰‹ï¼Œè¯·åŸºäºä»¥ä¸‹åŒ»å­¦èµ„æ–™å’Œå¯¹è¯å†å²å›ç­”é—®é¢˜ï¼š
{}

å½“å‰å¯¹è¯å†å²ï¼š
{}

å›ç­”è¦æ±‚ï¼š
1. æ ¸å¿ƒå®šä¹‰ç®€æ˜æ‰¼è¦
2. å…³é”®ç‰¹å¾åˆ†ç‚¹åˆ—å‡ºï¼ˆæœ€å¤š3ç‚¹ï¼‰
3. å¿…é¡»æ ‡æ³¨æ•°æ®æ¥æº
4. ä½¿ç”¨ä¸­æ–‡å£è¯­åŒ–è¡¨è¾¾
5. å¦‚é—®é¢˜æ¶‰åŠä¹‹å‰è®¨è®ºå†…å®¹ï¼Œè¯·ä¿æŒä¸€è‡´æ€§""".format(
            '\n'.join([f"[æ¥æº:{i + 1}] {text}" for i, text in enumerate(context)]),
            '\n'.join([f"Q: {h['question']}\nA: {h['answer']}" for h in self.dialog_history[-3:]])
        )
        return system_prompt

    def _enhance_query(self, question: str) -> str:
        keywords = self.disease_extractor.extract(question)
        if not keywords:
            return question

        enhanced = f"{question} {' '.join(keywords)}"

        for kw in keywords:
            enhanced += " " + " ".join(self.disease_extractor.medical_terms["synonyms"].get(kw, []))

        return enhanced

    def _retrieve_with_keywords(self, query: str, keywords: List[str]) -> List[dict]:
        return self.retriever.retrieve(query)

    def _extract_relevant_snippets(self, question: str, documents: List[dict]) -> List[dict]:
        all_sentences = []
        doc_reference = []
        for doc in documents:
            sentences = [s.strip() for s in re.split(r'(?<=[ã€‚ï¼ï¼Ÿï¼›\n])', doc["text"]) if s.strip()]
            all_sentences.extend(sentences)
            doc_reference.extend([doc["doc_id"]] * len(sentences))

        tokenized_sentences = [list(jieba.cut(s)) for s in all_sentences]
        self.bm25 = BM25Okapi(tokenized_sentences)

        tokenized_question = list(jieba.cut(question))
        top_n = min(5, len(all_sentences))
        scores = self.bm25.get_scores(tokenized_question)
        top_indices = np.argsort(scores)[-top_n:][::-1]

        snippets = []
        for idx in top_indices:
            if scores[idx] > 0:
                snippets.append({
                    "text": all_sentences[idx].strip(),
                    "doc_id": doc_reference[idx],
                    "score": float(scores[idx]),
                    "full_doc": documents[next(i for i, d in enumerate(documents) if d["doc_id"] == doc_reference[idx])]
                })
        return snippets

    def _generate_concise_answer(self, question: str, snippets: List[dict]) -> str:
        if not snippets:
            return "æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"

        context = [s["text"] for s in snippets[:3]]

        api_response = self.silicon_client.generate_answer(
            question=question,
            context=self._build_context_prompt(question, context)
        )

        if "choices" in api_response:
            full_answer = api_response["choices"][0]["message"]["content"]
            source_list = list({s["doc_id"] for s in snippets[:3]})
            return f"{full_answer}\n\næ•°æ®æ¥æºï¼š{', '.join(source_list)}"

        return "å¤©é—®æ¨¡å‹ç”Ÿæˆå›ç­”æ—¶å‘ç”Ÿé”™è¯¯"

    def _generate_exam_suggestions(self, diseases: List[str]) -> str:
        exam_map = {
            "ç³–å°¿ç—…": ["è¡€ç³–æ£€æµ‹", "ç³–åŒ–è¡€çº¢è›‹ç™½"],
            "å† å¿ƒç—…": ["å¿ƒç”µå›¾", "å† è„‰é€ å½±"],
            "è‚ºç‚": ["èƒ¸éƒ¨Xå…‰", "è¡€å¸¸è§„"],
            "é«˜è¡€å‹": ["è¡€å‹ç›‘æµ‹", "è¡€è„‚æ£€æŸ¥"],
            "è‚ç‚": ["è‚åŠŸèƒ½æ£€æŸ¥", "è‚ç‚ç—…æ¯’æ£€æµ‹"],
            "è‚¾ç‚": ["å°¿å¸¸è§„", "è‚¾åŠŸèƒ½æ£€æŸ¥"]
        }

        suggestions = set()
        for d in diseases:
            suggestions.update(exam_map.get(d, []))

        return "ã€".join(suggestions) if suggestions else "æš‚æ— å…·ä½“å»ºè®®"

    def _generate_full_document_response(self, question: str, snippets: List[dict]) -> str:
        if not snippets:
            return "æœªæ‰¾åˆ°ç›¸å…³åŒ»å­¦èµ„æ–™"

        response = f"## é—®é¢˜: {question}\n\n"
        response += "ä»¥ä¸‹æ˜¯ä¸æ‚¨é—®é¢˜æœ€ç›¸å…³çš„å®Œæ•´æ–‡æ¡£å†…å®¹:\n\n"

        unique_docs = {}
        for snippet in snippets:
            doc_id = snippet["doc_id"]
            if doc_id not in unique_docs:
                unique_docs[doc_id] = {
                    "text": snippet["full_doc"]["text"],
                    "score": snippet["score"],
                    "highlighted": []
                }
            unique_docs[doc_id]["highlighted"].append(snippet["text"])

        sorted_docs = sorted(unique_docs.items(), key=lambda x: x[1]["score"], reverse=True)

        for i, (doc_id, doc_data) in enumerate(sorted_docs[:3], 1):
            response += f"### æ–‡æ¡£ {i}: {doc_id} (ç›¸å…³æ€§: {doc_data['score']:.2f})\n\n"

            full_text = doc_data["text"]
            for highlight in doc_data["highlighted"]:
                full_text = full_text.replace(highlight, f"**{highlight}**")

            response += f"{full_text[:1000]}{'...' if len(full_text) > 1000 else ''}\n\n"

        return response

    def query(self, question: str, full_docs: bool = False) -> dict:
        try:
            is_follow_up = len(self.dialog_history) > 0 and "?" in question
            keywords = self.disease_extractor.extract(question)

            if is_follow_up:
                last_question = self.dialog_history[-1]["question"]
                enhanced_query = f"{last_question} {question} {' '.join(keywords)}"
            else:
                enhanced_query = self._enhance_query(question)

            retrieved_docs = self._retrieve_with_keywords(enhanced_query, keywords)

            if not retrieved_docs:
                return {
                    "answer": "æœªæ‰¾åˆ°ç›¸å…³åŒ»å­¦èµ„æ–™",
                    "sources": []
                }

            snippets = self._extract_relevant_snippets(question, retrieved_docs)

            if full_docs:
                answer = self._generate_full_document_response(question, snippets)
            else:
                answer = self._generate_concise_answer(question, snippets)

            self.dialog_history.append({
                "question": question,
                "answer": answer,
                "timestamp": datetime.now().isoformat(),
                "mode": "full" if full_docs else "concise"
            })
            if len(self.dialog_history) > self.max_history:
                self.dialog_history.pop(0)

            return {
                "answer": answer,
                "sources": [{
                    "doc_id": s["doc_id"],
                    "text": s["text"][:150] + "...",
                    "score": s["score"]
                } for s in snippets[:3]],
                "mode": "full" if full_docs else "concise"
            }

        except Exception as e:
            return {
                "answer": f"ç³»ç»Ÿå¤„ç†å‡ºé”™: {str(e)}",
                "sources": [],
                "mode": "error"
            }

    def clear_history(self):
        self.dialog_history = []


# ======================
# æ¨¡å‹åŠ è½½
# ======================
@st.cache_resource
def load_models():
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            SystemConfig.EMBEDDING_MODEL,
            cache_dir="./models"
        )
        model = AutoModel.from_pretrained(
            SystemConfig.EMBEDDING_MODEL,
            device_map=SystemConfig.EMBEDDING_DEVICE,
            cache_dir="./models"
        )
    except Exception as e:
        st.error(f"åµŒå…¥æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return None, None, None

    try:
        cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    except Exception as e:
        st.error(f"é‡æ’åºæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        return tokenizer, model, None

    return tokenizer, model, cross_encoder


# ======================
# Streamlitç•Œé¢ (å®Œæ•´ä¿®æ”¹ç‰ˆ)
# ======================
def main():
    st.set_page_config(
        page_title="åŒ»ç–—æ™ºèƒ½é—®ç­”ç³»ç»Ÿ",
        page_icon="ğŸ¥",
        layout="wide"
    )

    st.title("ğŸ¥ åŒ»ç–—æ™ºèƒ½é—®ç­”ç³»ç»Ÿ")

    if "rag_system" not in st.session_state:
        with st.spinner("ç³»ç»Ÿåˆå§‹åŒ–ä¸­..."):
            try:
                st.session_state.rag_system = MedicalRAGSystem()
                st.success("ç³»ç»Ÿå°±ç»ªï¼")
            except Exception as e:
                st.error(f"åˆå§‹åŒ–å¤±è´¥: {str(e)}")
                return

    with st.sidebar:
        st.header("âš™ï¸ ç³»ç»Ÿè®¾ç½®")

        enable_rerank = st.checkbox(
            "å¯ç”¨é«˜çº§é‡æ’åº",
            value=True,
            help="å¯ç”¨å¤šé˜¶æ®µé‡æ’åºä¼˜åŒ–æœç´¢ç»“æœè´¨é‡"
        )
        SystemConfig.RERANK_CONFIG["enable_rerank"] = enable_rerank

        if enable_rerank:
            st.subheader("é‡æ’åºæƒé‡é…ç½®")
            SystemConfig.RERANK_CONFIG["score_weights"]["vector"] = st.slider(
                "å‘é‡ç›¸ä¼¼åº¦æƒé‡", 0.1, 0.8, 0.5, 0.05
            )
            SystemConfig.RERANK_CONFIG["score_weights"]["keyword"] = st.slider(
                "å…³é”®è¯åŒ¹é…æƒé‡", 0.1, 0.8, 0.3, 0.05
            )
            SystemConfig.RERANK_CONFIG["score_weights"]["rerank"] = st.slider(
                "è¯­ä¹‰ç›¸å…³æƒé‡", 0.1, 0.8, 0.2, 0.05
            )

        st.header("ğŸ“‚ æ–‡æ¡£ç®¡ç†")
        data_dir = st.text_input("æ–‡æ¡£ç›®å½•è·¯å¾„", value=file_path)
        if st.button("åŠ è½½æ–‡æ¡£", help="ä»æŒ‡å®šç›®å½•åŠ è½½åŒ»ç–—æ–‡æ¡£"):
            if Path(data_dir).exists():
                files = []
                for ext in ("*.txt", "*.md", "*.html"):
                    files.extend(list(Path(data_dir).rglob(ext)))

                if files:
                    with st.spinner(f"æ­£åœ¨åŠ è½½ {len(files)} ä¸ªæ–‡æ¡£..."):
                        st.session_state.rag_system.retriever.ingest_documents([str(f) for f in files])
                    st.success("æ–‡æ¡£åŠ è½½å®Œæˆï¼")
                else:
                    st.warning("æœªæ‰¾åˆ°æ”¯æŒæ ¼å¼çš„æ–‡æ¡£")
            else:
                st.error("ç›®å½•ä¸å­˜åœ¨ï¼")

        st.header("ğŸ—¨ï¸ å¯¹è¯å†å²")
        if st.button("æ¸…ç©ºå¯¹è¯å†å²", help="æ¸…é™¤æ‰€æœ‰å¯¹è¯è®°å½•"):
            st.session_state.rag_system.clear_history()
            st.success("å¯¹è¯å†å²å·²æ¸…ç©º")

        st.header("â„¹ï¸ ç³»ç»Ÿä¿¡æ¯")
        st.markdown(f"""
        - å½“å‰å¯¹è¯æ•°: {len(st.session_state.rag_system.dialog_history)}
        - æœ€å¤§å†å²è®°å½•: {st.session_state.rag_system.max_history}
        - æœ€åæ›´æ–°æ—¶é—´: {datetime.now().strftime("%Y-%m-%d %H:%M")}
        """)

    col1, col2 = st.columns([3, 1])

    with col1:
        if hasattr(st.session_state.rag_system, 'dialog_history') and st.session_state.rag_system.dialog_history:
            st.subheader("ğŸ“œ å½“å‰å¯¹è¯å†å²")
            for i, dialog in enumerate(st.session_state.rag_system.dialog_history):
                with st.expander(f"å¯¹è¯{i + 1}: {dialog['question'][:30]}...",
                                 expanded=i == len(st.session_state.rag_system.dialog_history) - 1):
                    st.markdown(f"""
                    **é—®é¢˜**: {dialog['question']}  
                    **å›ç­”**: {dialog['answer']}  
                    *æ¨¡å¼: {dialog['mode']} | æ—¶é—´: {dialog['timestamp'][11:19]}*
                    """)
        else:
            st.info("ğŸ’¡ æš‚æ— å¯¹è¯å†å²ï¼Œè¯·è¾“å…¥é—®é¢˜å¼€å§‹å¯¹è¯")

        st.subheader("ğŸ’¬ è¯·è¾“å…¥åŒ»ç–—é—®é¢˜")
        question = st.text_area(
            "è¾“å…¥æ¡†",
            height=120,
            label_visibility="collapsed",
            placeholder="ä¾‹å¦‚ï¼šç³–å°¿ç—…çš„ç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿå¦‚ä½•æ²»ç–—ï¼Ÿ"
        )

        btn_col1, btn_col2 = st.columns(2)
        with btn_col1:
            if st.button("ğŸ¤– æ™ºèƒ½ç”Ÿæˆå›ç­”", help="ä»æ–‡æ¡£æå–å…³é”®ä¿¡æ¯ç”Ÿæˆç®€æ´å›ç­”", use_container_width=True):
                if question.strip():
                    with st.spinner("æ­£åœ¨æå–å…³é”®ä¿¡æ¯å¹¶ç”Ÿæˆå›ç­”..."):
                        result = st.session_state.rag_system.query(question, full_docs=False)

                        st.subheader("ğŸ©º ä¸“ä¸šå›ç­”")
                        st.markdown(result["answer"])

                        if result["sources"]:
                            with st.expander("ğŸ” å‚è€ƒç‰‡æ®µï¼ˆç‚¹å‡»å±•å¼€ï¼‰"):
                                for i, src in enumerate(result["sources"], 1):
                                    st.markdown(f"""
                                    **ç‰‡æ®µ{i}** (æ¥è‡ªæ–‡æ¡£ `{src['doc_id']}`)
                                    > {src['text']}
                                    """)
                    st.rerun()
                else:
                    st.warning("è¯·è¾“å…¥é—®é¢˜å†…å®¹")

        with btn_col2:
            if st.button("ğŸ“„ æ˜¾ç¤ºå®Œæ•´æ–‡æ¡£", help="æ˜¾ç¤ºåŸå§‹æ–‡æ¡£å†…å®¹", use_container_width=True):
                if question.strip():
                    with st.spinner("æ­£åœ¨æ£€ç´¢ç›¸å…³æ–‡æ¡£..."):
                        result = st.session_state.rag_system.query(question, full_docs=True)

                        st.subheader("ğŸ“š ç›¸å…³æ–‡æ¡£")
                        for i, doc in enumerate(result["sources"], 1):
                            with st.expander(f"æ–‡æ¡£{i}: {doc['doc_id']} (ç›¸å…³æ€§: {doc['score']:.2f})"):
                                st.markdown(f"""
                                **å†…å®¹**:  
                                {doc['text']}  
                                """)
                    st.rerun()
                else:
                    st.warning("è¯·è¾“å…¥é—®é¢˜å†…å®¹")

    with col2:
        st.subheader("ğŸš€ å¿«æ·è¿½é—®")
        if hasattr(st.session_state.rag_system, 'dialog_history') and st.session_state.rag_system.dialog_history:
            last_question = st.session_state.rag_system.dialog_history[-1]["question"]
            suggestions = [
                f"å…³äº'{last_question.split('?')[0]}'ï¼Œèƒ½å¦è¯¦ç»†è§£é‡Šä¸€ä¸‹ï¼Ÿ",
                "è¿™ä¸ªè¯Šæ–­éœ€è¦åšå“ªäº›æ£€æŸ¥ï¼Ÿ",
                "æœ‰å“ªäº›æ²»ç–—æ–¹æ³•ï¼Ÿ",
                "è¿™ä¸ªç—…çš„é¢„é˜²æªæ–½æ˜¯ä»€ä¹ˆï¼Ÿ",
                "è¿™äº›ç—‡çŠ¶ä¸¥é‡å—ï¼Ÿ",
                "éœ€è¦çœ‹ä»€ä¹ˆç§‘å®¤ï¼Ÿ"
            ]

            for i, suggestion in enumerate(suggestions):
                if st.button(suggestion, key=f"suggestion_{i}", use_container_width=True):
                    st.session_state.question_input = suggestion
                    st.rerun()
        else:
            st.info("è¾“å…¥ç¬¬ä¸€ä¸ªé—®é¢˜åï¼Œè¿™é‡Œä¼šæ˜¾ç¤ºè¿½é—®å»ºè®®")

        st.subheader("ğŸ·ï¸ ç›¸å…³ç–¾ç—…")
        if hasattr(st.session_state.rag_system, 'dialog_history') and st.session_state.rag_system.dialog_history:
            last_question = st.session_state.rag_system.dialog_history[-1]["question"]
            keywords = st.session_state.rag_system.disease_extractor.extract(last_question)
            if keywords:
                st.markdown("""
                <style>
                .tag {
                    display: inline-block;
                    background-color: #e6f2ff;
                    border-radius: 16px;
                    padding: 4px 12px;
                    margin: 4px;
                    font-size: 0.9em;
                }
                </style>
                """, unsafe_allow_html=True)

                cols = st.columns(2)
                for i, kw in enumerate(keywords):
                    with cols[i % 2]:
                        st.markdown(f'<span class="tag">{kw}</span>', unsafe_allow_html=True)
            else:
                st.info("æœªè¯†åˆ«åˆ°ç–¾ç—…å…³é”®è¯")


if __name__ == "__main__":
    main()
